{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1315\n"
     ]
    }
   ],
   "source": [
    "def count_sentences(conllu_file_path):\n",
    "    sentence_count = 0\n",
    "\n",
    "    with open(conllu_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip() == '':\n",
    "                sentence_count += 1\n",
    "\n",
    "    return sentence_count\n",
    "\n",
    "\n",
    "file_path = '/home/preetamray-pg/g2g-transformer/data/UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu'\n",
    "num_sentences = count_sentences(file_path)\n",
    "print(f\"Number of sentences: {num_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_head_conllu(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        # print(line)\n",
    "        if line.startswith('#'):\n",
    "            continue  # Skip comment lines\n",
    "        elif line.strip() == '':\n",
    "            continue  # Skip empty lines\n",
    "        else:\n",
    "            fields = line.split('\\t')\n",
    "            if fields[6] == '_':\n",
    "                count += 1\n",
    "                print(i+1)\n",
    "                # Insert a random integer ranging from 0 to the length of the line\n",
    "                # random_integer = 0\n",
    "                # fields[6] = str(random_integer)\n",
    "                lines[i] = '\\t'.join(fields)\n",
    "\n",
    "    # Update the file with modified lines\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_head_conllu(\"/home/preetamray-pg/G2GTr/data/gold/train.conllx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_root_at_last_or_second_last(conllu_sentence):\n",
    "    lines = conllu_sentence.split('\\n')\n",
    "\n",
    "    # Iterate through the lines to find the root\n",
    "    root_position = None\n",
    "    for line in lines:\n",
    "        if line and not line.startswith('#'):  # Ignore comments\n",
    "            columns = line.split('\\t')\n",
    "            if len(columns) >= 7 and columns[6] == '0':  # Check if it's a root\n",
    "                root_position = int(columns[0])\n",
    "\n",
    "    # Check if the root is at the last or second-last position\n",
    "    return root_position == len(lines) - 1 or root_position == len(lines) - 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly shuffled 'deps' column and saved to /home/preetam_pg/g2g-transformer/data/org/train-neg.conllu.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def read_conllu(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        conllu_data = file.read()\n",
    "    return conllu_data\n",
    "\n",
    "def write_conllu(file_path, conllu_data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(conllu_data)\n",
    "\n",
    "def shuffle_deprel_column(conllu_data):\n",
    "    # Split the data into lines\n",
    "    lines = conllu_data.strip().split('\\n')\n",
    "\n",
    "    # Parse each line and shuffle the deprel column\n",
    "    for i, line in enumerate(lines):\n",
    "        if not line.startswith('#'):\n",
    "            columns = line.split('\\t')\n",
    "            if len(columns) > 7:  # Ensure the line has enough columns\n",
    "                deprel_index = 7  # Assuming deprel is at column 8 (0-indexed)\n",
    "                deprels = columns[deprel_index].split('|')\n",
    "                random.shuffle(deprels)\n",
    "                columns[deprel_index] = '|'.join(deprels)\n",
    "                lines[i] = '\\t'.join(columns)\n",
    "\n",
    "    # Join the lines back into a single string\n",
    "    shuffled_conllu_data = '\\n'.join(lines)\n",
    "    return shuffled_conllu_data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = '/home/preetam_pg/g2g-transformer/data/org/train-pos.conllu'\n",
    "    output_file = '/home/preetam_pg/g2g-transformer/data/org/train-neg.conllu'\n",
    "\n",
    "    conllu_data = read_conllu(input_file)\n",
    "    shuffle_deprel_column(conllu_data)\n",
    "    write_conllu(output_file, conllu_data)\n",
    "\n",
    "    print(f\"Randomly shuffled 'deps' column and saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_deprels_on_each_word(conllu_sentence):\n",
    "    lines = conllu_sentence.strip().split('\\n')\n",
    "    shuffled_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        fields = line.split('\\t')\n",
    "        if len(fields) >= 8:  # Make sure there is a 7th column (deprel)\n",
    "            deprels = fields[7].split('|')\n",
    "            shuffled_deprels = random.sample(deprels, len(deprels))\n",
    "            fields[7] = '|'.join(shuffled_deprels)\n",
    "\n",
    "        shuffled_lines.append('\\t'.join(fields))\n",
    "\n",
    "    return '\\n'.join(shuffled_lines)\n",
    "\n",
    "def shuffle_deprels_in_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        conllu_data = infile.read()\n",
    "\n",
    "    shuffled_data = shuffle_deprels_on_each_word(conllu_data)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(shuffled_data)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = '/home/preetam_pg/g2g-transformer/data/org/train-pos.conllu'\n",
    "output_file_path = '/home/preetam_pg/g2g-transformer/data/org/train-neg.conllu'\n",
    "\n",
    "shuffle_deprel_in_file(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def collect_unique_deprels(conllu_data):\n",
    "    unique_deprels = set()\n",
    "\n",
    "    lines = conllu_data.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        fields = line.split('\\t')\n",
    "        if len(fields) >= 8:  # Make sure there is a 7th column (deprel)\n",
    "            deprels = fields[7].split('|')\n",
    "            unique_deprels.update(deprels)\n",
    "\n",
    "    return list(unique_deprels)\n",
    "\n",
    "def replace_deprels_with_random(conllu_sentence, unique_deprels):\n",
    "    lines = conllu_sentence.strip().split('\\n')\n",
    "    replaced_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        fields = line.split('\\t')\n",
    "        if len(fields) >= 8:  # Make sure there is a 7th column (deprel)\n",
    "            deprels = fields[7].split('|')\n",
    "            # print(deprels)\n",
    "            shuffled_deprel = random.choice(unique_deprels)\n",
    "            fields[7] = '|'.join([shuffled_deprel for _ in deprels])\n",
    "\n",
    "        replaced_lines.append('\\t'.join(fields))\n",
    "\n",
    "    return '\\n'.join(replaced_lines)\n",
    "\n",
    "def replace_deprels_in_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        conllu_data = infile.read()\n",
    "\n",
    "    unique_deprels = collect_unique_deprels(conllu_data)\n",
    "    # print(unique_deprels)\n",
    "    replaced_data = replace_deprels_with_random(conllu_data, unique_deprels)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(replaced_data)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_file_path = '/home/preetam_pg/g2g-transformer/data/org/train-pos.conllu'\n",
    "output_file_path = '/home/preetam_pg/g2g-transformer/data/org/train-neg.conllu'\n",
    "\n",
    "replace_deprels_in_file(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse, parse_tree\n",
    "import random\n",
    "\n",
    "def permute_sentence(sentence):\n",
    "    # Shuffle the word order\n",
    "    random.shuffle(sentence)\n",
    "\n",
    "    # Shuffle the deprel (dependency relations)\n",
    "    for token in sentence:\n",
    "        token['deprel'] = random.choice(token['deprel'])\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def permute_conllu_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Parse the CoNLL-U file\n",
    "    sentences = parse(data)\n",
    "\n",
    "    # Permute each sentence in the file\n",
    "    permuted_sentences = [permute_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "    # Convert back to CoNLL-U format\n",
    "    permuted_data = '\\n\\n'.join('\\n'.join(token_list.serialize()) for token_list in permuted_sentences)\n",
    "\n",
    "    # Save the permuted data to a new file\n",
    "    with open(\"/home/preetam_pg/g2g-transformer/data/org/train-pos.conllu\", 'w', encoding='utf-8') as file:\n",
    "        file.write(permuted_data)\n",
    "\n",
    "# Example usage\n",
    "permute_conllu_file('/home/preetam_pg/g2g-transformer/data/org/train.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled data written to /home/preetam_pg/g2g-transformer/data/org/train-pos.conllu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def read_conllu_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        conllu_data = file.read().strip().split('\\n\\n')\n",
    "    return conllu_data\n",
    "\n",
    "def shuffle_sentence(sentence):\n",
    "    lines = sentence.split('\\n')\n",
    "    random.shuffle(lines)  # Shuffle the lines to shuffle the word order\n",
    "    shuffled_sentence = '\\n'.join(lines)\n",
    "    return shuffled_sentence\n",
    "\n",
    "def shuffle_conllu_data(conllu_data):\n",
    "    shuffled_data = [shuffle_sentence(sentence) for sentence in conllu_data]\n",
    "    return shuffled_data\n",
    "\n",
    "def write_conllu_file(file_path, conllu_data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n\\n'.join(conllu_data))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file_path = '/home/preetam_pg/g2g-transformer/data/org/train.conllu'\n",
    "    output_file_path = '/home/preetam_pg/g2g-transformer/data/org/train-pos.conllu'\n",
    "\n",
    "    # Step 1: Read the CoNLL-U file\n",
    "    conllu_data = read_conllu_file(input_file_path)\n",
    "\n",
    "    # Step 2 and 3: Shuffle word order and relations\n",
    "    shuffled_data = shuffle_conllu_data(conllu_data)\n",
    "\n",
    "    # Step 4: Write shuffled data to a new CoNLL-U file\n",
    "    write_conllu_file(output_file_path, shuffled_data)\n",
    "\n",
    "    print(f\"Shuffled data written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "original_sentence = \"\"\"\n",
    "1   This    _   DET DT  _   2   det _   _\n",
    "2   is  _   VERB    VBZ _   0   root    _   _\n",
    "3   an  _   DET DT  _   4   det _   _\n",
    "4   example _   NOUN    NN  _   2   attr    _   _\n",
    "5   sentence    _   NOUN    NN  _   4   nsubj   _   _\n",
    "6   in  _   ADP IN  _   5   prep    _   _\n",
    "7   CoNLL-U _   PROPN   NN  _   6   pobj    _   _\n",
    "8   format  _   NOUN    NN  _   6   appos   _   _\n",
    "\"\"\"\n",
    "\n",
    "shuffled_sentence = shuffle_conllu(original_sentence)\n",
    "print(shuffled_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 6, 2: 1, 3: 8, 4: 2, 5: 7, 6: 3, 7: 4, 8: 5}\n",
      "6\taTa\t_\tind\t_\t_\t5\tsambanxah\t_\t_\n",
      "1\tsaH\t_\tnomsgm\t_\t_\t5\tkarwa\t_\t_\n",
      "8\tanugiram\t_\tind\t_\t_\t5\taxikaranam\t_\t_\n",
      "2\tftuBiH\t_\tinstplm\t_\t_\t7\tkarwa\t_\t_\n",
      "7\tvitAyamAnAM\t_\tpprmd\t_\t_\t3\tviseranam\t_\t_\n",
      "3\tvanaantalakzmIM\t_\taccsgf\t_\t_\t5\tkarma\t_\t_\n",
      "4\tvilokayituM\t_\tinf\t_\t_\t5\tprayojanam\t_\t_\n",
      "5\tniragamat\t_\taorsg3\t_\t_\t0\troot\t_\t_\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_conllu_sentence(sentence):\n",
    "    lines = sentence.strip().split('\\n')\n",
    "    tokens = [line.split('\\t') for line in lines if not line.startswith('#')]\n",
    "    # print(tokens)\n",
    "    \n",
    "    # Extract word indices\n",
    "    indices = [*range(1, len(tokens)+1, 1)]\n",
    "    # print(indices)\n",
    "    \n",
    "    # Shuffle indices\n",
    "    ind = [*range(1, len(indices)+1, 1)]\n",
    "    # print(ind)\n",
    "    shuffled_indices = random.sample(ind, len(ind))\n",
    "    # print(indices, shuffled_indices)\n",
    "    \n",
    "    # Create a mapping from old indices to shuffled indices\n",
    "    index_mapping = {old_index: new_index for old_index, new_index in zip(indices, shuffled_indices)}\n",
    "    print(index_mapping)\n",
    "    \n",
    "    # Update the tokens with shuffled indices\n",
    "    for token in tokens:\n",
    "        # update the head index\n",
    "        t = token[0]\n",
    "        old_index = int(t)\n",
    "        new_index = index_mapping[old_index]\n",
    "        token[0] = str(new_index)\n",
    "\n",
    "        # update the head dependency\n",
    "        t = token[6]\n",
    "        # print(t)\n",
    "        old_index = int(t)\n",
    "        if old_index != 0:\n",
    "            new_index = index_mapping[old_index]\n",
    "            token[6] = str(new_index)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Reconstruct the sentence without sorting tokens\n",
    "    shuffled_sentence = '\\n'.join(['\\t'.join(token) for token in tokens])\n",
    "    \n",
    "    return shuffled_sentence\n",
    "\n",
    "# Example usage\n",
    "conllu_sentence = \"\"\"\n",
    "1\taTa\t_\tind\t_\t_\t8\tsambanxah\t_\t_\n",
    "2\tsaH\t_\tnomsgm\t_\t_\t8\tkarwa\t_\t_\n",
    "3\tanugiram\t_\tind\t_\t_\t8\taxikaranam\t_\t_\n",
    "4\tftuBiH\t_\tinstplm\t_\t_\t5\tkarwa\t_\t_\n",
    "5\tvitAyamAnAM\t_\tpprmd\t_\t_\t6\tviseranam\t_\t_\n",
    "6\tvanaantalakzmIM\t_\taccsgf\t_\t_\t8\tkarma\t_\t_\n",
    "7\tvilokayituM\t_\tinf\t_\t_\t8\tprayojanam\t_\t_\n",
    "8\tniragamat\t_\taorsg3\t_\t_\t0\troot\t_\t_\n",
    "\"\"\"\n",
    "\n",
    "shuffled_conllu_sentence = shuffle_conllu_sentence(conllu_sentence)\n",
    "print(shuffled_conllu_sentence) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To use for random permutation of Prose to create Poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_conllu_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        sentences = infile.read().strip().split('\\n\\n')\n",
    "\n",
    "    shuffled_sentences = [shuffle_conllu_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write('\\n\\n'.join(shuffled_sentences))\n",
    "\n",
    "def shuffle_conllu_sentence(sentence):\n",
    "    lines = sentence.strip().split('\\n')\n",
    "    tokens = [line.split('\\t') for line in lines if not line.startswith('#')]\n",
    "    \n",
    "    indices = [*range(1, len(tokens)+1, 1)]\n",
    "    ind = [*range(1, len(indices)+1, 1)]\n",
    "    shuffled_indices = random.sample(ind, len(ind))\n",
    "    \n",
    "    index_mapping = {old_index: new_index for old_index, new_index in zip(indices, shuffled_indices)}\n",
    "    \n",
    "    # tokens.sort(key=lambda x: int(x[0]))\n",
    "\n",
    "    for token in tokens:\n",
    "        # Update the head index\n",
    "        t = token[0]\n",
    "        if '-' not in t:\n",
    "            old_index = int(t)\n",
    "            new_index = index_mapping[old_index]\n",
    "            token[0] = str(new_index)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Update the head dependency\n",
    "        t = token[6]\n",
    "        old_index = int(t)\n",
    "        if old_index != 0:\n",
    "            new_index = index_mapping[old_index]\n",
    "            token[6] = str(new_index)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    tokens.sort(key=lambda x: int(x[0]))\n",
    "    shuffled_sentence = '\\n'.join(['\\t'.join(token) for token in tokens])\n",
    "    \n",
    "    return shuffled_sentence\n",
    "\n",
    "# Example usage\n",
    "input_conllu_file = \"/home/preetamray-pg/g2g-transformer/data/UD_Turkish-Penn/tr_penn-ud-train.conllu\"\n",
    "output_conllu_file = \"/home/preetamray-pg/g2g-transformer/data/UD_Turkish-Penn/tr_penn-ud-train-pos.conllu\"\n",
    "shuffle_conllu_file(input_conllu_file, output_conllu_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_conllu(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            # print(line[0])\n",
    "            cols = line.split(\"\\t\")\n",
    "            if '.' in cols[0] or '-' in cols[0] or  '_\t_\t_\t_\t_\t_\t_' in line:\n",
    "                continue\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "            # if '.' in line[0]:\n",
    "            #         continue\n",
    "            # else:\n",
    "            #      outfile.write(line)\n",
    "\n",
    "# Example usage\n",
    "input_filename = '/home/preetamray-pg/g2g-transformer/data/UD_Marathi-UFAL/mr_ufal-ud-train.conllu'\n",
    "output_filename = '/home/preetamray-pg/g2g-transformer/data/UD_Marathi-UFAL/mr_ufal-ud-train-clean.conllu'\n",
    "filter_conllu(input_filename, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check exact match between prose and poetry of sentences of ramayana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def get_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = content.split('\\n')\n",
    "        # You may need to adjust the splitting logic based on your specific file format.\n",
    "        return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def compare_sentences(sentence1, sentence2):\n",
    "    words1 = sentence1.split()\n",
    "    words2 = sentence2.split()\n",
    "    matcher = difflib.SequenceMatcher(None, words1, words2)\n",
    "    return matcher.ratio()  # Returns a similarity ratio between 0 and 1.\n",
    "\n",
    "def main():\n",
    "    file1_path = '/home/preetamray-pg/g2g-transformer/data/ramayana/prose.txt'\n",
    "    file2_path = '/home/preetamray-pg/g2g-transformer/data/ramayana/poetry.txt'\n",
    "\n",
    "    sentences1 = get_sentences(file1_path)\n",
    "    sentences2 = get_sentences(file2_path)\n",
    "\n",
    "    for i, (sentence1, sentence2) in enumerate(zip(sentences1, sentences2), 1):\n",
    "        similarity_ratio = compare_sentences(sentence1, sentence2)\n",
    "        with open(\"prose_poetry_similarity.txt\", \"a\", encoding='UTF-8') as f:\n",
    "            f.writelines(f\"Comparison for sentence {i}: {similarity_ratio}\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract sentences from a conllu file and write to another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import random\n",
    "\n",
    "def extract_and_remove_sentences(input_file_path, output_file_path, num_sentences):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        data = input_file.read()\n",
    "\n",
    "    sentences = parse(data)\n",
    "    \n",
    "    # Ensure the number of sentences to extract is not greater than the total number of sentences\n",
    "    num_sentences = min(num_sentences, len(sentences))\n",
    "\n",
    "    # Randomly select sentences to extract\n",
    "    selected_sentences = random.sample(sentences, num_sentences)\n",
    "\n",
    "    # Write extracted sentences to a new file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for sentence in selected_sentences:\n",
    "            output_file.write(sentence.serialize())\n",
    "\n",
    "    # Remove extracted sentences from the original file\n",
    "    remaining_sentences = [s for s in sentences if s not in selected_sentences]\n",
    "\n",
    "    # Write the remaining sentences back to the original file\n",
    "    with open(input_file_path, 'w', encoding='utf-8') as input_file:\n",
    "        for sentence in remaining_sentences:\n",
    "            input_file.write(sentence.serialize())\n",
    "\n",
    "# Example usage:\n",
    "input_file_path = '/home/preetamray-pg/g2g-transformer/data/UD_Sanskrit-Vedic/sa_vedic-ud-train.conllu'\n",
    "output_file_path = '/home/preetamray-pg/g2g-transformer/data/UD_Sanskrit-Vedic/sa_vedic-ud-dev.conllu'\n",
    "num_sentences_to_extract = 500\n",
    "\n",
    "extract_and_remove_sentences(input_file_path, output_file_path, num_sentences_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
